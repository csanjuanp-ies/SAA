{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# U.T.6 Redes Neuronales.\n",
    "## Redes convolucionales (CNN)\n",
    "### Introducción\n",
    "Las CNN son muy populares al poder conseguir resultados muy interesantes en el reconocimiento de imágenes.\n",
    "\n",
    "Son capaces de aprender características de una imagen por sí solas a partir de los datos.\n",
    "\n",
    "Reconocen patrones locales en pequeñas partes de la imagen (en la imagen en una zona de 5x5) en vez de aprenderlos en\n",
    "toda la imagen.\n",
    "\n",
    "Las redes suponen que la entrada será una imagen, con lo que la dimensión de los datos será un tensor 3D: alto, ancho\n",
    "y profundidad (o canales de color).\n",
    "\n",
    "![](img/ut06_25.png)\n",
    "\n",
    "Cada capa abstrae o reconoce un nivel diferente, en la primera parecerán bordes, en la segunda formas simples, en los\n",
    "siguientes patrones más complejos\n",
    "\n",
    "![](img/ut06_26.png)\n",
    "\n",
    "Las CNN realizan su trabajo muy bien por dos ideas fundamentales de las mismas:\n",
    "- Escasa conectividad. Un mapa de características está conectado un pequeño grupo de píxeles. Además, estos patrones\n",
    "son invariantes a las traslaciones.\n",
    "- Compartición de parámetros. Los mismos pesos se comparten entre diferentes grupos de píxeles.\n",
    "\n",
    "Y estas ideas funcionan porque generalmente en una imagen los píxeles cercanos tendrán información relacionada entre\n",
    "ellos en vez de aquellos que están mas alejados. Además, las imágenes se forman por jerarquías de patrones, que se van\n",
    "reconociendo según vamos avanzando por las capas convolucionales.\n",
    "\n",
    "![](img/ut06_27.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0239 - accuracy: 0.9929\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "0.992900013923645\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "=================================================================\n",
      "Total params: 55,744\n",
      "Trainable params: 55,744\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras.datasets import mnist\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "# LAS NUEVAS CAPAS\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu', input_shape=(28, 28, 1)))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "model.add(layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 26, 26, 32)        320       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 13, 13, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 11, 11, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 5, 5, 64)          0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 3, 3, 64)          36928     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                36928     \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                650       \n",
      "=================================================================\n",
      "Total params: 93,322\n",
      "Trainable params: 93,322\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# CAPAS YA CONOCIDAS\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(10, activation='softmax'))\n",
    "print(model.summary())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
    "print(train_images.shape)  # (60000, 28, 28)  solo un valor, monocromo\n",
    "# convertir a la convención example,height,width,channel\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "train_images = train_images.astype('float32') / 255  # normalizar\n",
    "test_images = test_images.reshape((10000, 28, 28, 1))\n",
    "test_images = test_images.astype('float32') / 255"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "# necesitamos que los labels con one-hot encode\n",
    "print(train_labels[0])  # 5\n",
    "train_labels = to_categorical(train_labels)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n"
     ]
    },
    {
     "data": {
      "text/plain": "<keras.callbacks.History at 0x1fd46eb9ac0>"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(train_labels[0])  # [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
    "test_labels = to_categorical(test_labels)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64, verbose=0)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 0.0303 - accuracy: 0.9902\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\n",
      "0.9901999831199646\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Generalmente las CNN están compuestas por varias capas convolucionales y capas de submuestreo, seguidas por una o\n",
    "más capas densas al final. Las capas de submuestreo (pooling) no tienen parámetros a entrenar, ni tienen pesos ni valor\n",
    "de varianza, mientras que las capas convolucionales sí.\n",
    "\n",
    "Las capa inicial convolucional necesita de entrada (height, width, channels) en el ejemplo anterior:\n",
    "input_shape=(28, 28, 1).\n",
    "\n",
    "La salida de cada capa convolucional y maxpoling es un tensor de dimension 3D\n",
    "(height, width, channels_fiters)  (conv2d (Conv2D)    (None, 26, 26, 32)).  El número de canales o filtros.\n",
    "\n",
    "El número de canales de salida se controla con el primer parámetro de las capas (32, o 64)[ layers.Conv2D(32,..]\n",
    "\n",
    "La última capa convolucional sirve de entrada a una capa densa con el mismo número de entradas, por lo que hay que\n",
    "aplanarla antes con flatter.\n",
    "\n",
    "El tamaño de entrada se reduce por el problema de padding, si queremos el mismo debemos ajustarlo en la creación de\n",
    "la capa.\n",
    "\n",
    "La capa de salida se define con la función Softmax y con el número de clases a predecir."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La operación de Convolución\n",
    "Cada capa de convolución aprende patrones locales dentro de la imagen en pequeñas ventanas de dos dimensiones,\n",
    "no en toda la imagen.\n",
    "\n",
    "El propósito principal de una capa es detectar rasgos visuales. Una vez aprenda una característica será capaz de\n",
    "reconocerla en cualquier parte de la imagen.\n",
    "\n",
    "Una capa es capaz de reconocer jerarquías de características utilizando lo aprendido por las capas superiores.\n",
    "Estas jerarquías serán objetos, disposiciones de objetos, etc. Por lo que las CNN son capaces de aprender patrones\n",
    "visuales complejos y de forma eficiente\n",
    "\n",
    "La operación de convolución opera sobre tensores en 3D exclusivamente en los datos de entrada, con lo que será imprescindible esta transformación antes de introducir los datos\n",
    "\n",
    "train_images = train_images.reshape((60000, 28, 28, 1))\n",
    "\n",
    "Los tensores expresan alto, ancho y número de canales de color (1 para monocromo y tres o cuatro para color).\n",
    "Como vemos a la red se le proporciona una dimensión más con los ejemplos\n",
    "\n",
    "las etiquetas multiclase deben estar codificadas con el mecanismo one-hot (una etiqueta por columna)\n",
    "\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "### La operación de Convolución\n",
    "La capa CNN es muy diferente a la densa estudiada anteriormente, ya que esta conecta cada neurona con una parte\n",
    "muy pequeña de la imagen (3x3 o 5x5) en vez de toda la imagen (esa ventana se denomina kernel).\n",
    "\n",
    "![](img/ut06_28.png)\n",
    "\n",
    "Como vemos en la imagen, la matriz de 5x5 se conectará con la primera neurona de la siguiente capa, si desplazamos\n",
    "esta matriz una posición, se conectará con la segunda neurona, así hasta terminar la fila. Una vez finalizada se\n",
    "desplaza la matriz una fila y a la primera columna y empezamos para la segunda fila, de esta manera se recorrerá toda\n",
    "la imagen para conectar la segunda capa a la primera).\n",
    "\n",
    "Este movimiento de desplazamiento genera dos problemas, el primero que la dimensionalidad de la capa de salida será\n",
    "menor que la de entrada (se gestiona con el **padding**) y segundo, la cantidad de desplazamiento de la ventana puede ser\n",
    "variable, en el ejemplo ha sido uno, pero se gestiona con el **stride**.\n",
    "\n",
    "![](img/ut06_29.png)\n",
    "\n",
    "Como cada neurona está conectada con un grupo pequeño de neuronas de la capa anterior (5x5 en el ejemplo) solo tendrá\n",
    "una matriz de pesos de ese tamaño (25 elementos), esta matriz es compartida por toda la capa y se ajustará durante el\n",
    "aprendizaje. Es decir, se usa la misma matriz para hacer todo el recorrido de la imagen\n",
    "\n",
    "![](img/ut06_30.png)\n",
    "\n",
    "![](img/ut06_31.png)\n",
    "\n",
    "El **padding** es la opción de agregar ceros alrededor para no perder dimensionalidad.\n",
    "\n",
    "El **stride** es el desplazamiento del kernel para recorrer la imagen.\n",
    "\n",
    "Para una entrada de 5x5 con un kernel de 3x3 y un stride de (1,1) se genera un filtro de 3x3.\n",
    "\n",
    "![](img/ut06_30.png)\n",
    "\n",
    "#### Ejemplo\n",
    "\n",
    "![](img/ut06_32.png)\n",
    "\n",
    "En el ejemplo el padding es cero y que en cada operación el vector w se desplaza dos celdas a la derecha (stride=2,2).\n",
    "Estos dos parámetros (p y s) tendremos que ajustarlos en nuestras capas convolucionales. El desplazamiento que debe\n",
    "sufrir el kernel en cada operación se denomina stride y es 1,\n",
    "\n",
    "La matriz de salida de la operación se denomina filtro, y es el resultado de mover el kernel por toda la imagen,\n",
    "multiplicar las dos matrices de forma escalar y sumar los resultados\n",
    "\n",
    "El problema es que cada filtro solo reconoce una característica, con lo que es necesario repetir este procedimiento\n",
    "varias veces (32, 64) para la misma capa para que tenga en cuenta varias características, el número de filtros a definir\n",
    "en una capa se establecen al crear la capa.\n",
    "\n",
    "model.add(layers.Conv2D(32, (3, 3), activation='relu'),\n",
    "  input_shape=(28, 28, 1))\n",
    "\n",
    "El conjunto de filtros de una capa se denomina espacio de características o **feature space**.\n",
    "\n",
    "![](img/ut06_33.png)\n",
    "\n",
    "#### Consideraciones finales\n",
    "Para trabajar con las redes convolucionales tendremos las siguientes características\n",
    "- Imagen de entrada con una dimensión alto, ancho, canales\n",
    "- A La imagen se le aplicará un kernel de tamaño 3x3 o 5x5\n",
    "- Se le podrá añadir un número de columnas y filas de ceros en los extremos para evitar la reducción de la dimensión\n",
    "de salida, se llama padding.\n",
    "- El kernel se trasladará a lo largo de toda la imagen con saltos de stride, generalmente igual a 1 en todas las\n",
    "direcciones\n",
    "- Se multiplicará el valor de cada celda de la imagen con la correspondiente del kernel que cae encima y se sumará el\n",
    "valor de todas las celdas resultantes\n",
    "- Se repetirla el paso anterior hasta cubrir toda la imagen dando otro vector de salida llamado filtro\n",
    "- El conjunto de todos los filtros es el espacio de características\n",
    "\n",
    "### La Operación de Submuestreo (pooling)\n",
    "El submuestreo realizan una simplificación de la información recogida por la capa de convolución y crea una versión\n",
    "condesada de la información de la misma reduciendo la dimensionalidad. Se deben aplicar justo después de cada capa\n",
    "de convolución.\n",
    "\n",
    "![](img/ut06_34.png)\n",
    "\n",
    "model.add(layers.MaxPooling2D((2, 2)))\n",
    "\n",
    "El submuestreo, adopta dos formas: max-min o mean. En la primera opción se elige el mayor de todos los implicados y\n",
    "en la segunda se hace la media.\n",
    "\n",
    "El funcionamiento es similar, se establece un tamaño de kernel que se aplica a la entrada, se desplaza a través de\n",
    "todos los valores y se va calculando el resultado final.\n",
    "\n",
    "La salida será un vector de menor tamaño que la entrada en función de la dimensión del kernel (pooling size).\n",
    "\n",
    "![](img/ut06_35.png)\n",
    "\n",
    "Como la capa convolucional tiene varios filtros (32, 64) el pooling se aplicará a cada uno de ellos generando tantas\n",
    "salidas como filtros haya.\n",
    "\n",
    "![](img/ut06_36.png)\n",
    "\n",
    "### Imágenes a color\n",
    "Si trabajamos con imágenes en gris solo existirá una matriz con valores, pero si estamos usando colores estas aumentas\n",
    "hasta 3 como mínimo (RGB) o incluso cuatro (RGBA) siendo la última la capa de transparencia.\n",
    "\n",
    "En caso de usar imágenes a color, el procedimiento es el mismo, lo único que se repetirá tantas veces como canales\n",
    "de entrada tengamos (RGB) y se sumarán las tres capas usando una matriz de suma que determine el modo que se pondera\n",
    "dicha suma. Cada capa tiene su propio kernel.\n",
    "\n",
    "El tratamiento de imágenes hace que se necesita gran cantidad de memoria y de cálculo, por lo que deberemos cargar\n",
    "siempre nuestras imágenes como uint8.\n",
    "\n",
    "### Hiperparámetros: Tamaño y número de filtros\n",
    "El tamaño del kernel determinará el tamaño del filtro, a mayor tamaño más información se perderá. Los tamaños\n",
    "recomendados son 3x3 y 5x5, pudiendo aumentar si la dimensionalidad de la imagen de entrada es muy grande.\n",
    "\n",
    "Del mismo modo el número de filtros determinan el número de características a usar, estos valores suelen ser 32 o 64,\n",
    "pero se podrá aumentar o disminuir en función de la complejidad del problema.\n",
    "\n",
    "### Hiperparámetros: Padding\n",
    "El pading es el número de columnas y filas rellenas de ceros que se aumentará el tensor inicial para mantener la\n",
    "dimensión en el tensor de salida. Recordemos que al aplicar un filtro de tamaño mayor de 1x1 la salida reducirá su\n",
    "dimensionalidad.\n",
    "\n",
    "![](img/ut06_37.png)\n",
    "\n",
    "![](img/ut06_38.png)\n",
    "\n",
    "Los valores que podemos usar en el parámetro padding son:\n",
    "- Full. p = m-1, generalmente no se usa.\n",
    "- Same. Asegura que el vector de salida tiene el mismo tamaño que el de entrada. Es la opción más usada.\n",
    "- Finally o Valid. Hace que p=0 y ser reduzca el tamaño del vector de salida, es el defecto en Keras.\n",
    "\n",
    "### Hiperparámetros: Stride\n",
    "Es el número de pasos que se desplaza el kernel hacia la izquierda y hacia abajo en el desplazamiento por la imagen.\n",
    "Este valor suele ser una matriz indicando los pasos en cada sentido, pero por norma general no se utiliza y se usa\n",
    "por defecto la matriz (1,1).\n",
    "\n",
    "### Hiperparámetros: Regularización: Dropout y L1, L2\n",
    "Generalmente se usa un porcentaje del 50% en las capas Dropout. El efecto es forzar a la capa a aprender patrones\n",
    "más generales y robustos, mejorando además el sobreajuste. Hay que tener cuidado que no se deben poner tras la capa\n",
    "de entrada."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dropout_layer = keras.layers.Dropout(0.5)\n",
    "\n",
    "conv_layer = keras.layers.Conv2D( filters=16, kernel_size=(3, 3), kernel_regularizer=keras.regularizers.l1(0.001))\n",
    "\n",
    "fc_layer = keras.layers.Dense( units=16, kernel_regularizer=keras.regularizers.l2(0.001))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hiperparámetros: BatchNormalization\n",
    "Esta regularización tiene el mismo efecto que el estudiado en el curso con la normalización, lleva las entradas\n",
    "dentro de una media de cero y una desviación de 1. Estas capas aparecen justo antes de la capa Dropout y después de\n",
    "una Densa o convolucional."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dropout_layer = keras.layers.BatchNormalitation()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hiperparámetros: Regularización: Decaimiento del ratio de aprendizaje\n",
    "Las redes CNN también se aprovechan de ellas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# opción uno\n",
    "optimizer = keras.optimizers.SGD(learning_rate=0.2, momentum=0.9, decay=0.01)\n",
    "model.compile(.. optimizer=optimizer)\n",
    "\n",
    "# opción dos\n",
    "reduce_lr = LearningRateScheduler(lambda x: 1e-1*0.9**x)\n",
    "model.fit(train_images, train_labels, epochs=5, batch_size=64,\n",
    "\tcallbacks=[reduce_lr])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Hiperparámetros: Resumen\n",
    "-Conv2D: tf.keras.layers.Conv2D\n",
    "    -Filters. Número de características a encontrar. Suelen ir creciendo en orden de múltiplos de dos hasta las capas densas.\n",
    "    -kernel_size. El tamaño es muy importante y no se debe usar grandes tamaños (5x5 o más) en cada capa, la única excepción es la primera capa que se suele utilizar con un tamaño de 5x5 y un stride de dos.\n",
    "    -strides, depende de la imagen, si son pequeñas (64x64) a cero\n",
    "    -padding, generalmente a SAME.\n",
    "-MaxPool2D: tf.keras.layers.MaxPool2D\n",
    "    -pool_size (2 para hace que se reduzca a la mitad el tamaño)\n",
    "    -strides. No se suele establecer.\n",
    "    -Padding. No se suele establecer.\n",
    "-Normalización: tf.keras.layers.Normalization(), opcional\n",
    "-Dropout: tf.keras.layers.Dropout2D\n",
    "    -Rate (hasta 50%)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "\tkeras.layers.Conv2D(64, 7, activation=\"relu\", padding=\"same\",\n",
    "\tinput_shape=[28, 28, 1]),\n",
    "\tkeras.layers.BatchNormalization(),\n",
    "\tkeras.layers.MaxPooling2D(2),\n",
    "\tkeras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "\tkeras.layers.Conv2D(128, 3, activation=\"relu\", padding=\"same\"),\n",
    "\tkeras.layers.MaxPooling2D(2),\n",
    "\tkeras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "\tkeras.layers.Conv2D(256, 3, activation=\"relu\", padding=\"same\"),\n",
    "\tkeras.layers.MaxPooling2D(2),\n",
    "\tkeras.layers.Flatten(),\n",
    "\tkeras.layers.Dense(128, activation=\"relu\"),\n",
    "\tkeras.layers.Dropout(0.5),\n",
    "\tkeras.layers.Dense(64, activation=\"relu\"),\n",
    "\tkeras.layers.Dropout(0.5),\n",
    "\tkeras.layers.Dense(10, activation=\"softmax\")\n",
    "])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transferencia del aprendizaje\n",
    "Hay gran cantidad de modelos para el reconocimiento cada uno de ellos con sus ventajas y sus inconvenientes\n",
    "\n",
    "La idea principal es aprovechar lo que otros ya han creado para nuestro beneficio. Existen las siguientes técnicas\n",
    "- Transformación de imágenes\n",
    "- Uso de modelos ya entrenados\n",
    "- Extracción de características\n",
    "- Afinado de modelos\n",
    "\n",
    "#### Transferencia del aprendizaje: Transformación de imágenes\n",
    "Se pretende generar más datos de entrenamiento a partir de nuestros datos aplicando transformaciones aleatorias a la\n",
    "imagen de entrada, produciendo otras similares, pero creibles\n",
    "\n",
    "No generar imágenes que nunca podrían encontrase en realidad\n",
    "\n",
    "Estas transformaciones las realizazá la clase ImageGenerator al vuelo, sin necesidad de almacenarlas en disco"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "for fn in os.listdir(train_cats_dir):\n",
    "    path = os.path.join(train_cats_dir, fn)\n",
    "    img = load_img(path)\n",
    "    data = img_to_array(img)\n",
    "    samples = expand_dims(data, 0)\n",
    "\n",
    "    # example of \"rotation_range\"\n",
    "    datagen = ImageDataGenerator(rotation_range=45)\n",
    "\n",
    "    it = datagen.flow(samples, batch_size=1)\n",
    "    for i in range(6):\n",
    "        plt.subplot(230 + 1 + i)\n",
    "        batch = it.next()\n",
    "        image = batch[0].astype('uint8')\n",
    "        plt.imshow(image)\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos a transformar la imagen que se va a pasar a la red cada epoch de tal manera que la red nunca reciba\n",
    "la misma imagen. Hay que recordar que en cada época se leen de nuevo todos los ejemplos que tenemos.\n",
    "\n",
    "Multiplicaremos el número de imágenes por el número de épocas\n",
    "\n",
    "Estos datos no se guardan, con lo que con cada ejecución variarán los datos de entrada"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "modelDA = Sequential()\n",
    "modelDA.add(Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)))\n",
    "modelDA.add(MaxPooling2D(2, 2))\n",
    "modelDA.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "modelDA.add(MaxPooling2D(2, 2))\n",
    "modelDA.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "modelDA.add(MaxPooling2D(2, 2))\n",
    "modelDA.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "modelDA.add(MaxPooling2D(2, 2))\n",
    "modelDA.add(Flatten())\n",
    "modelDA.add(Dense(512, activation='relu'))\n",
    "modelDA.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "modelDA.compile(loss='binary_crossentropy',\n",
    "                optimizer=RMSprop(lr=1e-4),\n",
    "                metrics=['acc'])\n",
    "#"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Creamos los transformadores\n",
    "Solo se crean nuevas imágenes en las de train, nunca en los otros conjuntos, en los que simplemente se escalan y\n",
    "normalizan como siempre con las imágenes."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1. / 255,\n",
    "        rotation_range=40,\n",
    "        width_shift_range=0.2,\n",
    "        height_shift_range=0.2,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True,\n",
    "        fill_mode='nearest')\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1.0 / 255.)\n",
    "test_datagen = ImageDataGenerator(rescale=1.0 / 255.)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(train_dir,\n",
    "                       batch_size=20,\n",
    "                       class_mode='binary',\n",
    "                       target_size=(150, 150))\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "\t validation_dir, batch_size=20,\n",
    "\t class_mode='binary',target_size=(150, 150))\n",
    "test_generator = test_datagen.flow_from_directory(test_dir,\n",
    "\t batch_size=20,class_mode='binary',\n",
    "\t target_size=(150, 150))\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Al trabajar con directorios y no con datos cargados, tenemos que crear una estructura de datos que vaya\n",
    "proporcionándolos a la red según los mini-batch elegidos, en Python crearemos generadores sobre el contenido de\n",
    "los directorios (flow_from_directory) a partir de los objetos generadores de imágenes\n",
    "\n",
    "Por último, ya estamos en concidciones de entrenar y validar el modelo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "historyDA = modelDA.fit(\n",
    "        train_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=100,\n",
    "        validation_data=validation_generator,\n",
    "        validation_steps=validation_steps,\n",
    "        verbose=2)\n",
    "\n",
    "print(steps_per_epoch)\n",
    "print(validation_steps)\n",
    "test_lost, test_acc = modelDA.evaluate(test_generator)\n",
    "print(\"Test Accuracy:\", test_acc)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Transferencia del aprendizaje: Uso de modelos ya entrenados\n",
    "En lugar de entrenar una red completamente, podemos usar una entrenada para nuestros fines. Hay dos aproximaciones\n",
    "usarlo directamente, o modificar las capas de la red entrenada.\n",
    "\n",
    "Es muy simple hacer uso de un modelo ya entrenado, simplemente lo cargamos, preparamos las imágenes al tamaño que\n",
    "espera el modelo, procesamos dichas imágenes y realizamos la predicción."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model = keras.applications.resnet50.ResNet50(weights=\"imagenet\")\n",
    "images_resized = tf.image.resize(images, [224, 224])\n",
    "inputs = keras.applications.resnet50.preprocess_input(\n",
    "\t\timages_resized * 255)\n",
    "Y_proba = model.predict(inputs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Transferencia del aprendizaje: Extracción de características\n",
    "En un modelo ya entrenado fijamos algunas capas y reentrenamos con nuestros ejemplos del problema en particular que\n",
    "queremos resolver mejorando los tiempos de entrenamiento y el rendimiento final\n",
    "\n",
    "Hay que recordar que las CNN aprenden de lo más simple a lo más complejo, con lo que si vamos eliminando capas desde\n",
    "la salida, nos quedaremos con los datos que generalizan bien para todas las clases, en nuestro ejemplo de animales\n",
    "\n",
    "En este tipo vamos a liberar el modelo del discriminante final, las capas densas (include_top=False)\n",
    "\n",
    "Congelamos todas las capas del modelo entrenado, creamos nuestro clasificador y creamos el modelo nuevo compuesto de\n",
    "ambos, para pasar a entrenarlo después"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_trained_model = VGG16(input_shape=(150, 150, 3),\n",
    "                          include_top=False,\n",
    "                          weights='imagenet')\n",
    "pre_trained_model.summary()\n",
    "\n",
    "modelFE = Sequential()\n",
    "modelFE.add(pre_trained_model)\n",
    "modelFE.add(Flatten())\n",
    "modelFE.add(Dense(256, activation='relu'))\n",
    "modelFE.add(Dense(1, activation='sigmoid'))  # para la clasificación binaria\n",
    "modelFE.summary()\n",
    "\n",
    "modelFE.compile(loss='binary_crossentropy', optimizer=RMSprop(lr=1e-4), metrics=['acc'])\n",
    "\n",
    "batch_size = 20\n",
    "steps_per_epoch = train_generator.n // batch_size\n",
    "validation_steps = validation_generator.n // batch_size\n",
    "\n",
    "historyFE = modelFE.fit(train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=100,\n",
    "        validation_steps=validation_steps,\n",
    "        verbose=2)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Transferencia del aprendizaje: Afinado del modelo\n",
    "Un ajuste más acertado generalmente es en el que además de eliminar el clasificador, no se fijan todas las capas del\n",
    "modelo que quedan, solo las más altas, para que se ajusten también a nuestro problema las últimas capas\n",
    "convolucionales.\n",
    "\n",
    "Para que funcione correctamente, deberemos establecer ritmos de aprendizajes lentos para evitar que cambien mucho los\n",
    "parámetros ya aprendidos en las capas no fijas convolucionales."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pre_trained_model = VGG16(input_shape=(150, 150, 3),\n",
    "                          include_top=False,\n",
    "                          weights='imagenet')\n",
    "\n",
    "pre_trained_model.trainable = True\n",
    "set_trainable = False\n",
    "\n",
    "for layer in pre_trained_model.layers:\n",
    "    if layer.name == 'block5_conv1':  # solo el block_5 se entrenará\n",
    "        set_trainable = True\n",
    "    if set_trainable:\n",
    "        layer.trainable = True\n",
    "    else:\n",
    "        layer.trainable = False\n",
    "pre_trained_model.summary()\n",
    "\n",
    "modelFT = Sequential()\n",
    "modelFT.add(pre_trained_model)\n",
    "modelFT.add(Flatten())\n",
    "modelFT.add(Dense(256, activation='relu'))\n",
    "modelFT.add(Dense(1, activation='sigmoid'))\n",
    "modelFT.summary()\n",
    "modelFT.compile(loss='binary_crossentropy',\n",
    "                optimizer=RMSprop(lr=1e-4),\n",
    "                metrics=['acc'])\n",
    "historyFT = modelFT.fit(\n",
    "        train_generator,\n",
    "        validation_data=validation_generator,\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        epochs=100,\n",
    "        validation_steps=validation_steps,\n",
    "        verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Transferencia del aprendizaje: Modelos ya implementados\n",
    "- densenet module: DenseNet models for Keras.\n",
    "- efficientnet module: EfficientNet models for Keras.\n",
    "- inception_resnet_v2 module: Inception-ResNet V2 model for Keras.\n",
    "- inception_v3 module: Inception V3 model for Keras.\n",
    "- mobilenet module: MobileNet v1 models for Keras.\n",
    "- mobilenet_v2 module: MobileNet v2 models for Keras.\n",
    "- mobilenet_v3 module: MobileNet v3 models for Keras.\n",
    "- nasnet module: NASNet-A models for Keras.\n",
    "- resnet module: ResNet models for Keras.\n",
    "- resnet_v2 module: ResNet v2 models for Keras.\n",
    "- vgg16 module: VGG16 model for Keras.\n",
    "- vgg19 module: VGG19 model for Keras.\n",
    "- xception module: Xception V1 model for Keras.\n",
    "\n",
    "![](img/ut06_39.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Clasificación y localización de objetos\n",
    "La localización de objetos en una imagen puede entenderse como un problema de regresión.\n",
    "\n",
    "Para predecir el marco que encierra el objeto, la aproximación más general es intentar añadir el centro del objeto\n",
    "y la anchura y altura de la caja que lo encierra.\n",
    "\n",
    "Esta aproximación hace que haya que predecir cuatro números más, pero que no implica un cambio importante,\n",
    "simplemente hay que añadir una capa Densa con cuatro unidades entrenadas con MSE\n",
    "\n",
    "El problema se plantea a la hora de crear los datos de entrenamiento: tenemos muchos datos con etiquetas, pero hay\n",
    "muy pocos datos con límites"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_model = keras.applications.xception.Xception(\n",
    "\t weights=\"imagenet\",include_top=False)\n",
    "avg = keras.layers.GlobalAveragePooling2D()(base_model.output)\n",
    "class_output = keras.layers.Dense( n_classes, activation=\"softmax\")(avg)\n",
    "loc_output = keras.layers.Dense(4)(avg)\n",
    "optimizer = keras.optimizers.SGD(Learning_rate=0.01, momentum=0.9,\n",
    "                       nesterov=True, decay=0.001)\n",
    "model = keras.Model(inputs=base_model.input,\n",
    "\t outputs=[class_output, loc_output])\n",
    "model.compile(loss=[\"sparse_categorical_crossentropy\", \"mse\"],\n",
    "\tloss_weights=[0.8, 0.2], # depends on what you care most about\n",
    "\toptimizer=optimizer, metrics=[\"accuracy\"])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Redes Convolucionales Completas\n",
    "La redes FCN se introdujeron para tareas de segmentación semántica. La idea es cambiar la última capa densa por una\n",
    "covolucional, de tal manera que el tamaño del filtro tiene que ser igual al tamaño de la entrada de los mapas de\n",
    "características y usar VALID para el padding, además el parámetro stride tiene que ser a 1.\n",
    "La característica principal es que las FCN contiene solo capas convolucionales y permite que la red sea entrenada\n",
    "en imágenes de cualquier tamaño.\n",
    "\n",
    "La elección de la arquitectura de detección dependerá del problema.\n",
    "- YOLO. Esta arquitectura se propuso para la detección de objetos en las imágenes y en sus diferentes versiones se han\n",
    "ido mejorando paulatinamente.\n",
    "- SSD\n",
    "- Faster-RCNN\n",
    "\n",
    "### Segmentación semántica\n",
    "La segmentación semántica intenta clasificar los pixeles de una imagen de acuerdo a una clase, detectando la clase,\n",
    "no la individualidad, con lo que si hay dos coches en la imagen juntos aparecerán los dos como coches y no como dos\n",
    "objetos separados\n",
    "\n",
    "Existen varias arquitecturas para este problema y las más sencillas se basan en una nueva capa Conv2DTranspose()\n",
    "\n",
    "![](img/ut06_40.png)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}