{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# U.T. 2. Aprendizaje supervisado (I).\n",
    "# Introducción\n",
    "\n",
    "No existe ningún algoritmo que se ajuste a todos los posibles escenarios.\n",
    "Hay que probar varios algoritmos y comparar el rendimiento entre ellos para elegir el que mejor se adapte.\n",
    "Hay que usar métricas semejantes en todos los algoritmos.\n",
    "\n",
    "La librería scikit-learn ofrece una gran variedad de algoritmos de aprendizaje ya implementados junto con cantidad de\n",
    "funciones para prepocesar los datos, optimizar los algoritmos y evaluar los modelos\n",
    "\n",
    "El procedimiento es el siguiente:\n",
    "1. Elegir las características y recoger los ejemplos etiquetados.\n",
    "2. Procesar los datos y dividirlos en grupos.\n",
    "3. Elegir la métrica.\n",
    "4. Elegir un algoritmo de clasificación y optimización.\n",
    "5. Evaluar el rendimiento del modelo.\n",
    "6. Optimizar el algoritmo cambiando los hiperparámetros, pasar al punto 5 hasta que sea óptimo"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Perceptron\n",
    "### Preparación de los datos\n",
    "Los pasos necesarios para la preparación de los datos son:\n",
    "1. Elección de los datos\n",
    "2. Conversión de las características categóricas en valores numéricos\n",
    "3. Dividir el conjunto de datos de entrada en datos de entrenamiento y datos de test\n",
    "    - Se divide en dos conjuntos los datos, unos se utilizará para entrenar el algoritmo.\n",
    "    - El segundo se guarda para las métricas.\n",
    "    - El segundo tendrá un tamaño entre 25% y 30%.\n",
    "    - La división se hará semejante por clases (y).\n",
    "\n",
    "`X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)`\n",
    "\n",
    "![](img/ut02_00.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### train_test_split\n",
    "`train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)`\n",
    "\n",
    "Divide dos conjuntos de datos según las características que indiquemos\n",
    "- **test_size** indica el porcentaje del segundo grupo de datos.\n",
    "- **stratify=y** indica que cada conjunto tenga un porcentaje similar según las clases o etiquetas (y).\n",
    "- **random_state** se utiliza para poder reproducir resultados, es el valor de inicialización de generador aleatorio.\n",
    "- Esta función mezcla los datos al hacer los grupos, por lo que no tendremos que hacerlo nosotros.\n",
    "\n",
    "***Es importante recordar que los ejemplos se deben suministrar de forma aleatoria si queremos mejorar la convergencia***"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Estandarización\n",
    "El orden de magnitud de las variables influye en los procedimientos estadísticos.\n",
    "\n",
    "Por ejemplo, si tenemos una muestra donde se recogen el peso y la altura de ciertos individuos es importante las\n",
    "unidades en cómo se miden las variables. Si la altura la cuantificamos en metros, los individuos posiblemente estarían\n",
    "en el intervalo [0; 2] mientras que el peso, si fuese medido en kilogramos, en el intervalo [0; 200].\n",
    "Una variable tiene un rango 100 veces más grande que la otra.\n",
    "\n",
    "La estandarización:\n",
    "- Mejora la convergencia.\n",
    "- No es imprescindible pero hay que probarla.\n",
    "- Se debe realizar con los datos de entrenamiento y aplicar a los demás datos.\n",
    "\n",
    "<code>\n",
    "sc = StandardScaler()<br>\n",
    "sc.fit(X_train)<br>\n",
    "X_train_std = sc.transform(X_train)<br>\n",
    "X_test_std = sc.transform(X_test)<br>\n",
    "</code>\n",
    "\n",
    "### Entrenamiento y métrica\n",
    "Las métricas se usan para comparar rendimientos.\n",
    "Existen gran cantidad de métricas.\n",
    "El algoritmo incorpora métricas propias.\n",
    "Existen métricas externas.\n",
    "La métrica nos puede dar el error producido o la exactitud del mismo.\n",
    "El uso en la métrica del error producido frente a la exactitud del algoritmo es cuestión de gustos, ya que ambos\n",
    "están relacionados según: exactitud = 1 – error.\n",
    "\n",
    "<code>\n",
    "ppn = Perceptron(eta0=0.1, random_state=1)<br>\n",
    "ppn.fit(X_train_std, y_train)<br>\n",
    "print(\"Tasa:\", ppn.score(X_test_std, y_test))<br>\n",
    "y_pred = ppn.predict(X_test_std)<br>\n",
    "print(\"Tasa:\", accuracy_score(y_test, y_pred))<br>\n",
    "</code>\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de las etiquetas [0 1 2]\n",
      "Número de etiquetas por casos: [50 50 50] [35 35 35] [15 15 15]\n",
      "Tasa: 0.9777777777777777\n",
      "Tasa: 0.9777777777777777\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "print(\"Valores de las etiquetas\", np.unique(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                    random_state=1, stratify=y)\n",
    "print(\"Número de etiquetas por casos:\", np.bincount(y),\n",
    "                       np.bincount(y_train),np.bincount(y_test))\n",
    "# escalado: Normalización. Recordar que el ajuste se hace sobre los datos de\n",
    "#       entrenamiento y la transformación se aplica a ambos, prueba y entrenamiento\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "ppn = Perceptron(eta0=0.1, random_state=1)\n",
    "ppn.fit(X_train_std, y_train)\n",
    "print(\"Tasa:\", ppn.score(X_test_std, y_test))\n",
    "\n",
    "# predecir datos y comparar con los originales\n",
    "y_pred = ppn.predict(X_test_std)\n",
    "print(\"Tasa:\", accuracy_score(y_test, y_pred))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](img/ut02_01.png)\n",
    "\n",
    "**Probar a no estandarizar los datos**\n",
    "\n",
    "![](img/ut02_02.png)\n",
    "![](img/ut02_03.png)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Sobreajuste\n",
    "Uno de los problemas principales es el **sobreajuste**.\n",
    "Los datos se ajustan muy bien a los datos de entrenamiento pero no a los de test\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regresión logística\n",
    "Es uno de los métodos más utilizados en la industria, es un algoritmo de clasificación aunque su nombre indica regresión.\n",
    "Es un modelo de clasificación binaria que se puede generalizar para múltiples clases: Regresión logística multinomial o\n",
    "Softmax.\n",
    "\n",
    "Conceptos matemáticos\n",
    "- Se usa la función sigmoide como función de activación.\n",
    "- La función sigmoide recoge un entero y lo transforma en real, y= 0,5 Cuando x=0.\n",
    "- El resultado es la probabilidad de que un ejemplo pertenezca a una clase concreta.\n",
    "- Es muy importante el valor de la certeza: cuánto estamos seguros de que la predicción sea correcta.\n",
    "- Hay que minimizar la función sigmoide para usar el descenso de gradiente estocástico (SGD)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de las etiquetas [0 1 2]\n",
      "Número de etiquetas por casos: [50 50 50] [35 35 35] [15 15 15]\n",
      "Tasa: 0.9777777777777777\n",
      "Tasa: 0.9777777777777777\n",
      "[[3.81527885e-09 1.44792866e-01 8.55207131e-01]\n",
      " [8.34020679e-01 1.65979321e-01 3.25737138e-13]\n",
      " [8.48831425e-01 1.51168575e-01 2.62277619e-14]]\n",
      "Clases de las tres primeras filas: [2 0 0]\n",
      "Clase de la fila 3: [0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "print(\"Valores de las etiquetas\", np.unique(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,\n",
    "                                                                 random_state=1, stratify=y)\n",
    "print(\"Número de etiquetas por casos:\", np.bincount(y),\n",
    "                       np.bincount(y_train),np.bincount(y_test))\n",
    "# escalado: Normalización. Recordar que el ajuste se hace sobre los datos de\n",
    "#       entrenamiento y la transformación\n",
    "#  se aplica a ambos, prueba y entrenamiento\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "lrn = LogisticRegression(C=100, random_state=1, solver='lbfgs', multi_class='ovr')\n",
    "# C valores menores mayor ajuste, se verá más adelante el concepto\n",
    "# solver = liblinear para clasifiación binaria y lbfgs para multiclase\n",
    "# multi_class = ovr hace que se utilice ese algoritmo para la división entre clases\n",
    "#    asigna 1 a la clase positiva y al resto 0, hace tantas características nuevas como\n",
    "#    necesite\n",
    "lrn.fit(X_train_std, y_train)\n",
    "print(\"Tasa:\", lrn.score(X_test_std, y_test))\n",
    "y_pred = lrn.predict(X_test_std)\n",
    "print(\"Tasa:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# predecimos ahora los tres primeros\n",
    "print(lrn.predict_proba(X_test_std[:3, ]))\n",
    "# [[3.81527885e-09 1.44792866e-01 8.55207131e-01]     85% en la clase tercera\n",
    "#  [8.34020679e-01 1.65979321e-01 3.25737138e-13]     83% en la primera clase\n",
    "#  [8.48831425e-01 1.51168575e-01 2.62277619e-14]]    84% en la primera clase\n",
    "# Clases predichas:\n",
    "print(\"Clases de las tres primeras filas:\", lrn.predict(X_test_std[:3, :]))  # [2 0 0]\n",
    "print(\"Clase de la fila 3:\", lrn.predict(X_test_std[2:3, :]))  # [0]\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](img/ut02_04.png)\n",
    "\n",
    "### Parámetros de la regresión logística\n",
    "- **C** (float), default=1.0. Es la inversa de la regularización, debe ser un valor real positivo. Valores más pequeños implican mayor regularización\n",
    "- **solver**{‘newton-cg’, ‘lbfgs’, ‘liblinear’, ‘sag’, ‘saga’}, default=’lbfgs’. Algoritmo a usar para realizar el ajuste.\n",
    "    - Para pequeños dataset y problemas binarios se utiliza liblinear si el dataset es grande mejor usar ‘sag’ y ‘saga’ que son más rápidos\n",
    "    - Para problemas con más de dos clases se tiene que usar ‘newton-cg’, ‘sag’, ‘saga’ y ‘lbfgs’\n",
    "    - Los algoritmos ‘newton-cg’, ‘lbfgs’, ‘sag’ y ‘saga’  pueden usarse con la regularización L2\n",
    "    - Los algoritmos ‘liblinear’ y ‘saga’ permiten también a regularización L1\n",
    "    - Los algoritmos ‘sag’ y ‘saga’ convergerán solo si las características están aproximadamente en la misma escala.\n",
    "- **multi_class**{‘auto’, ‘ovr’, ‘multinomial’}, default=’auto’. La opción ovr se debe usar para problemas de multiclass en los que cada característica es binaria. La opción multinomial no está disponible para el solver liblinear\n",
    "- **n_jobs** int, default=None. Número de CPUs a usar de forma paralela. Se usará solo si se elige multi_class=’ovr’, es ignorado para ‘liblinear’. -1 significa usar todos los procesadores posibles.\n",
    "- **penalty**{‘l1’, ‘l2’, ‘elasticnet’, ‘none’}, default=’l2’. Usado para determinar la regularización a usar.\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Regularización\n",
    "Generalmente se pretende reducir la capacidad del modelo y / o reducción de la varianza de las predicciones para mejorar\n",
    "la convergencia\n",
    "\n",
    "La regularización puede ser entendida como el proceso de agregar información (cambiar la función objetivo) para evitar\n",
    "el sobreajuste\n",
    "\n",
    "### Qué es el sobreajuste\n",
    "Problema común en el ML.\n",
    "Significa que el modelo se ajusta muy bien a los datos de entrenamiento, pero no también a datos nuevos (test).\n",
    "Solución: reducir la complejidad del modelo: por ejemplo regularizando, hay más opciones.\n",
    "\n",
    "![](img/ut02_05.png)\n",
    "\n",
    "- En azul la curva de ajuste para los datos de prueba\n",
    "- En naranja la misma curva para los datos de test\n",
    "- Eje Y el error, eje X número de épocas\n",
    "- Podemos ver cómo el error no cambia mucho en los datos de test (val_loss)\n",
    "- Vemos que la diferencia se amplía\n",
    "\n",
    "\n",
    "### Qué es el subajuste\n",
    "Problema común en el ML.\n",
    "El modelo no es lo suficiente complejo para capturar la estructura de los datos\n",
    "Solución: aumentar la complejidad del modelo.\n",
    "\n",
    "![](img/ut02_06.png)\n",
    "\n",
    "- En azul la curva de ajuste para los datos de prueba\n",
    "- En naranja la misma curva para los datos de test\n",
    "- Eje Y el error, eje X número de épocas\n",
    "- Podemos ver cómo el error no cambia mucho en los datos de test (val_loss) pero permanece constante\n",
    "\n",
    "### Sin errores\n",
    "- Azul datos de prueba, naranja datos de test\n",
    "- Una gráfica es error, la otra es rendimiento, ver cómo son complementarias\n",
    "\n",
    "![](img/ut02_07.png)\n",
    "\n",
    "### Equilibrio tendencia-varianza (bias-variance)\n",
    "Se utiliza para describir el rendimiento de un modelo.\n",
    "\n",
    "Una alta varianza es proporcional a un sobreajuste. La varianza mide la consistencia del modelo y es sensible al orden\n",
    "de los datos de entrada.\n",
    "\n",
    "La tendencia mide cómo de lejos estás las predicciones de la realidad y no es susceptible al orden o a diferentes\n",
    "conjuntos de entrenamiento.\n",
    "\n",
    "No se puede conseguir la mínima varianza y la mínima tendencia, están relacionadas, hay que llegar a un equilibrio."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Máquinas vector soporte\n",
    "Pueden ser consideradas una extensión del perceptron.\n",
    "Se basan en minimizar los errores de clasificación maximizando el margen. El **margen** se define como la distancia\n",
    "entre el hiperplano de separación (bordes de decisión) y los ejemplos que están más cerca al mismo que se denominan:\n",
    "vectores de soporte.\n",
    "La idea es maximizar el margen, hacerlo lo más grande posible.\n",
    "\n",
    "![](img/ut02_08.jpg)\n",
    "\n",
    "El parámetro C se comporta como un control de la penalización. En concreto valores pequeños de C dan valores grandes\n",
    "de penalización en la clasificación de los errores y viceversa.\n",
    "\n",
    "En definitiva, se puede controlar la anchura del margen con el parámetro C. C pequeño, margen grande y viceversa.\n",
    "\n",
    "El parámetro C lo hemos visto anteriormente: LogisticRegression(C=100…\n",
    "\n",
    "![](img/ut02_09.png)\n",
    "\n",
    "![](img/ut02_10.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores de las etiquetas [0 1 2]\n",
      "Número de etiquetas por casos: [50 50 50] [35 35 35] [15 15 15]\n",
      "Tasa: 0.9777777777777777\n",
      "Tasa: 0.9777777777777777\n",
      "[[0.00772374 0.01640841 0.97586785]\n",
      " [0.92249209 0.05497364 0.02253427]\n",
      " [0.95813714 0.02701406 0.0148488 ]]\n",
      "Clases de las tres primeras filas: [2 0 0]\n",
      "Clase de la fila 3: [0]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data[:, [2, 3]]\n",
    "y = iris.target\n",
    "print(\"Valores de las etiquetas\", np.unique(y))\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1, stratify=y)\n",
    "print(\"Número de etiquetas por casos:\", np.bincount(y), np.bincount(y_train), np.bincount(y_test))\n",
    "\n",
    "# escalado: Normalización. Recordar que el ajuste se hace sobre los datos de entrenamiento y la transformación\n",
    "#  se aplica a ambos, prueba y entrenamiento\n",
    "sc = StandardScaler()\n",
    "sc.fit(X_train)\n",
    "X_train_std = sc.transform(X_train)\n",
    "X_test_std = sc.transform(X_test)\n",
    "\n",
    "\n",
    "svm = SVC(kernel='linear', C=1.0, random_state=1, probability=True)\n",
    "svm.fit(X_train_std, y_train)\n",
    "\n",
    "print(\"Tasa:\", svm.score(X_test_std, y_test))\n",
    "y_pred = svm.predict(X_test_std)\n",
    "print(\"Tasa:\", accuracy_score(y_test, y_pred))\n",
    "\n",
    "# predecimos ahora los tres primeros\n",
    "print(svm.predict_proba(X_test_std[:3, ]))\n",
    "# [[0.00772374 0.01640841 0.97586785]     97%\n",
    "#  [0.92249209 0.05497364 0.02253427]     92%\n",
    "#  [0.95813714 0.02701406 0.0148488 ]]     95%\n",
    "# Clases predichas:\n",
    "print(\"Clases de las tres primeras filas:\", svm.predict(X_test_std[:3, :]))  # [2 0 0]\n",
    "print(\"Clase de la fila 3:\", svm.predict(X_test_std[2:3, :]))  # [0]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](img/ut02_11.png)\n",
    "\n",
    "\n",
    "### Parmámetros\n",
    "- **C** (float), default=1.0. Es la inversa de la regularización, debe ser un valor real positivo. Valores más pequeños implican mayor regularización y margen más grande (recordar que se busca maximizar el margen).\n",
    "- **kernel**{‘linear’, ‘poly’, ‘rbf’, ‘sigmoid’, ‘precomputed’}, default=’rbf’. Algoritmo a usar para realizar el ajuste.\n",
    "- **degree** int, default=3. Coeficiente del polinomio a usar si el kernel es poly\n",
    "- **coef0** float, default=0.0. Valor del término independiente si el kernel es poly\n",
    "- **probability** bool, default=False. Indica si hay que estimar la probabilidad de una clase\n",
    "- **gamma**{‘scale’, ‘auto’} or float, default=’scale’. Coeficiente gamma para rbf, poly y sigmoid\n",
    "\n",
    "### Problemas no lineales\n",
    "\n",
    "La razón de que MVS sea tan popular es por la posibilidad de usar diferentes kernels para solucionar los problemas\n",
    "de clasificación que no son linealmente separables.\n",
    "\n",
    "La idea principal es crear combinaciones no lineales de las características originales y proyectarlas a un espacio\n",
    "dimensional mayor que sea separable linealmente.\n",
    "\n",
    "El problema de construir esas características es que son computacionalmente muy costosas.\n",
    "\n",
    "![](img/ut02_12.png)\n",
    "\n",
    "#### Problemas no lineales: rbf\n",
    "Uno de los kernels más usado es el RBF (Radial basic function) o kernel Gausiano. γ (gamma) es un parámetro libre\n",
    "para optimizar. Establecido a un valor pequeño hará que los bordes del área sean más suaves,\n",
    "mientras que un valor mayor los convertirá en más abruptos.\n",
    "\n",
    "<code>\n",
    "svc = SVC(kernel=’rbf’, random_state=1, gamma=0.2, C=1.0)<br>\n",
    "svc = SVC(kernel=’rbf’, random_state=1, gamma=100, C=1.0)<br>\n",
    "</code>\n",
    "\n",
    "#### Problemas no lineales: polinomial\n",
    "Otra aproximación es añadir más características en forma de características polinomiales como combinación lineal de\n",
    " las características iniciales\n",
    "\n",
    "<code>\n",
    "svc = SVC(kernel=’poly’, degree=3, coef0=1, C=1.0)<br>\n",
    "</code>"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Árboles de decisión\n",
    "Un árbol de decisión es un algoritmo que divide los datos en función de una serie de preguntas.\n",
    "Se empieza desde la raíz y se va dividiendo los datos de las características buscando la mayor ganancia de información (Ig).\n",
    "Este proceso se hace de forma iterativa durante la división de los datos. Si no ponemos límites a la iteración llegaremos\n",
    "al sobreajuste.\n",
    "\n",
    "![](img/ut02_13.png)\n",
    "\n",
    "### Algoritmos para el parámetro cryterion\n",
    "Se busca maximizar la función objetivo, hay tres algoritmos\n",
    "- **Entropía**. Esta función es máxima cuando la distribución es uniforme de los ejemplos y cero cuando todos los\n",
    "ejemplos de un nodo son de la misma clase.\n",
    "- **Gini**. Se puede entender como un criterio para minimizar la probabilidad de errores mal clasificados.\n",
    "Es máxima si las clases están perfectamente mezcladas.\n",
    "- **Error de clasificación**. Se utiliza sobre todo para realzar la poda del árbol en vez de para su creación.\n",
    "\n",
    "De los tres criterios anteriores, los dos primeros funcionan de forma similar.\n",
    "Generalmente el mecanismo Gini está en medio entre la Entropía y el Error de clasificación.\n",
    "No es necesario escalar los datos para este mecanismo."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "tree = DecisionTreeClassifier(criterion='gini', max_depth=2, random_state=1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parámetros\n",
    "- **criterion**{\"gini”, \"entropy”}, default=”gini”. Algoritmo a usar\n",
    "- **splitter**{\"best”, \"random”}, default=”best”. Estrategía para divider cada nodo.\n",
    "- **max_depth** int, default=None. Máxima profundidad del árbol, si es muy grando aparecerá el sobreajuste\n",
    "- **min_samples_split** int or float, default=2. Número mínimo de ejemplos para divider un nodo.\n",
    "- **min_samples_leaf** int or float, default=1. Mínimo número de ejemplos que tiene que haber en un nodo hoja\n",
    "\n",
    "![](img/ut02_14.png)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Bosques aleatorios\n",
    "Un bosque utiliza un conjunto de árboles como base para realizar las decisiones y separar los ejemplos.\n",
    "El parámetro más importante es el número de árboles (K).\n",
    "La mayor ventaja es que no hay que preocuparse por los hiperparámetros. En general, Cuantos más árboles más rendimiento\n",
    "en el clasificador, pero más coste computacional."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(criterion='gini', n_estimators= 25, random_state=1, n_jobs=-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "![](img/ut02_15.png)\n",
    "\n",
    "### Parámetros\n",
    "- **n_estimators** int, default=100. Número de árboles a usar\n",
    "- **criterion**{\"gini”, \"entropy”}, default=”gini”. Algoritmo a us\n",
    "- **splitter**{\"best”, \"random”}, default=”best”. Estrategía para divider cada nodo.\n",
    "- **max_depth** int, default=None. Máxima profundidad del árbol, si es muy grando aparecerá el sobreajuste\n",
    "- **min_samples_split** int or float, default=2. Número mínimo de ejemplos para divider un nodo.\n",
    "- **min_samples_leaf** int or float, default=1. Mínimo número de ejemplos que tiene que haber en un nodo hoja"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## K-Vecinos\n",
    "Este algoritmo hace conjuntos de k-vecinos basándose en la distancia que nosotros elijamos.\n",
    "Se adapta muy bien a recoger datos de prueba on-line, pero el coste computacional se incrementará de forma lineal con\n",
    "el número de nuevos casos.\n",
    "La correcta elección del parámetro k (número de vecinos n_neighbors en el código anterior) es crucial para un buen\n",
    "funcionamiento, así como de la métrica.\n",
    "Generalmente la distancia euclídea es una buena aproximación, pero deberemos estandarizar los datos\n",
    "\n",
    "![](img/ut02_16.png)\n",
    "\n",
    "### Algoritmo\n",
    "1. Elegir el número de k vecinos y la métrica de distancia.\n",
    "2. Encontrar los k-vecinos más cercanos en función de la métrica.\n",
    "3. Asignar la clase en función del voto mayoritario.\n",
    "\n",
    "Existen dos parámetros claves, el número de vecinos (k) y la métrica. Si la métrica incluye algún otro parámetro\n",
    "también será necesario establecerlo.\n",
    "\n",
    "### Métricas\n",
    "|Identificador|Nombre de clase|args|Función de distancia|\n",
    "|-------------|---------------|----|--------------------|\n",
    "|\"euclidean”|EuclideanDistance|-|sqrt(sum((x - y)^2))|\n",
    "|\"manhattan”|ManhattanDistance|-|sum(|x - y|)|\n",
    "|\"chebyshev”|ChebyshevDistance|-|max(|x - y|)|\n",
    "|\"minkowski”|MinkowskiDistance|p|sum(|x - y|^p)^(1/p)|\n",
    "|\"wminkowski”|WMinkowskiDistance|p, w|sum(|w * (x - y)|^p)^(1/p)|\n",
    "|\"seuclidean”|SEuclideanDistance|V|sqrt(sum((x - y)^2 / V))|\n",
    "|\"mahalanobis”|MahalanobisDistance|V or VI|sqrt((x - y)' V^-1 (x - y))|\n",
    "\n",
    "### Problema\n",
    "Si los vecinos están muy separados entre sí en un espacio de alta dimensionalidad.\n",
    "En este caso se recurren a las técnicas de reducción de dimensionalidad o a la reducción de características (PCA)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "knn = KNeighborsClassifier(n_neighbors=5, p=2, metric='minkowski')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Parámetros\n",
    "- **n_nvighbors** int, default=5. Número de vecinos a usar por grupo.\n",
    "- **p** int, default=2. Potencia de la métrica minkowski.\n",
    "- **metric** str. Cadena con el nombre de la métrica."
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}